{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we shal train a recurrent neural network on the [Cornell Movie dataset](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded the dataset!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Create the data folder if it doesn't exist\n",
    "if not os.path.exists(\"./data/\"):\n",
    "    os.makedirs('./data/')\n",
    "\n",
    "# Download the dataset\n",
    "url = 'http://www.mpi-sws.org/~cristian/data/cornell_movie_dialogs_corpus.zip'\n",
    "file_name = './data/{}'.format(os.path.basename(url))\n",
    "\n",
    "request = urlopen(url)\n",
    "data = request.read()\n",
    "\n",
    "with open(file_name, 'wb') as file:\n",
    "    file.write(data)\n",
    "\n",
    "# Unzip the dataset\n",
    "with ZipFile(file_name, 'r') as zf:\n",
    "    with zf.open('cornell movie-dialogs corpus/movie_conversations.txt', 'r') as source:\n",
    "        with open('./data/movie_conversations.txt', 'wb') as target:\n",
    "            target.write(source.read())\n",
    "    with zf.open('cornell movie-dialogs corpus/movie_lines.txt', 'r') as source:\n",
    "        with open('./data/movie_lines.txt', 'wb') as target:\n",
    "            target.write(source.read())\n",
    "\n",
    "# Delete the zip file\n",
    "os.remove(file_name)\n",
    "\n",
    "print('Downloaded the dataset!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['L194', 'L195', 'L196', 'L197'], ['L198', 'L199'], ['L200', 'L201', 'L202', 'L203'], ['L204', 'L205', 'L206'], ['L207', 'L208']]\n"
     ]
    }
   ],
   "source": [
    "# Get all the sequence of conversations from the dataset\n",
    "with open('./data/movie_conversations.txt', 'rt') as file:\n",
    "    movie_conversations = [line.split(' +++$+++ ') for line in file.read().split('\\n')]\n",
    "    movie_conversations = [eval(line[3]) for line in movie_conversations if len(line) == 4]\n",
    "    \n",
    "print(movie_conversations[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['L1045', 'They do not!'], ['L1044', 'They do to!'], ['L985', 'I hope so.'], ['L984', 'She okay?'], ['L925', \"Let's go.\"]]\n"
     ]
    }
   ],
   "source": [
    "# Get all the movie lines from the dataset\n",
    "with open('./data/movie_lines.txt', 'rt') as file:\n",
    "    movie_lines = [line.split(' +++$+++ ') for line in file.read().split('\\n')]\n",
    "    movie_lines = [[line[0], line[4]] for line in movie_lines if len(line) == 5]\n",
    "\n",
    "print(movie_lines[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('L390978', 'My son does not like this Johnny Cammareri. He says he is a big baby.'), ('L3688', 'Okay, sweetheart. Have a lovely Birthday Party tomorrow.'), ('L274331', \"Hey, your chili's getting cold --\"), ('L540517', 'Do you know the name of the Captain of this vessel?'), ('L233911', 'So what about your story. You thought of a title yet?')]\n"
     ]
    }
   ],
   "source": [
    "# Transform the movie lines in a dictionary by the line ID\n",
    "lines_dict = {id: line for id, line in movie_lines}\n",
    "\n",
    "print(list(lines_dict.items())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['they', 'do', 'not', '<EXCLAMATION_MARK>', 'they', 'do', 'to', '<EXCLAMATION_MARK>', 'i', 'hope', 'so', '<PERIOD>', 'she', 'okay', '<QUESTION_MARK>', \"let's\", 'go', '<PERIOD>', 'wow', 'okay', '<HYPHENS>', \"you're\", 'gonna', 'need', 'to', 'learn', 'how', 'to', 'lie', '<PERIOD>', 'no', \"i'm\", 'kidding', '<PERIOD>', 'you', 'know', 'how', 'sometimes', 'you', 'just', 'become', 'this', '<QUOTATION_MARK>', 'persona', '<QUOTATION_MARK>', '<QUESTION_MARK>', 'and', 'you', \"don't\", 'know', 'how', 'to', 'quit', '<QUESTION_MARK>', 'like', 'my', 'fear', 'of', 'wearing', 'pastels', '<QUESTION_MARK>', 'the', '<QUOTATION_MARK>', 'real', 'you', '<QUOTATION_MARK>', '<PERIOD>', 'what', 'good', 'stuff', '<QUESTION_MARK>', 'i', 'figured', \"you'd\", 'get', 'to', 'the', 'good', 'stuff', 'eventually', '<PERIOD>', 'thank', 'god', '<EXCLAMATION_MARK>', 'if', 'i', 'had', 'to', 'hear', 'one', 'more', 'story', 'about', 'your', 'coiffure', '<PERIOD>', '<PERIOD>', '<PERIOD>', 'me', '<PERIOD>', 'this', 'endless', '<PERIOD>', '<PERIOD>', '<PERIOD>', 'blonde', 'babble', '<PERIOD>', \"i'm\", 'like', '<COMMA>', 'boring', 'myself', '<PERIOD>', 'what', 'crap', '<QUESTION_MARK>', 'do', 'you', 'listen', 'to', 'this', 'crap', '<QUESTION_MARK>', 'no', '<PERIOD>', '<PERIOD>', '<PERIOD>', 'then', 'guillermo', 'says', '<COMMA>', '<QUOTATION_MARK>', 'if', 'you', 'go', 'any', 'lighter', '<COMMA>', \"you're\", 'gonna', 'look', 'like', 'an', 'extra', 'on', '90210', '<PERIOD>', '<QUOTATION_MARK>', 'you', 'always', 'been', 'this', 'selfish', '<QUESTION_MARK>', 'but', 'then', \"that's\", 'all', 'you', 'had', 'to', 'say', '<PERIOD>', 'well', '<COMMA>', 'no', '<PERIOD>', '<PERIOD>', '<PERIOD>', 'you', 'never', 'wanted', 'to', 'go', 'out', 'with', \"'me\", '<COMMA>', 'did', 'you', '<QUESTION_MARK>', 'i', 'was', '<QUESTION_MARK>', 'i', 'looked', 'for', 'you', 'back', 'at', 'the', 'party', '<COMMA>', 'but', 'you', 'always', 'seemed', 'to', 'be']\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all text\n",
    "all_lines = '\\n'.join(line[1] for line in movie_lines)\n",
    "\n",
    "# Lower the case for all words and replace tokens\n",
    "all_lines = all_lines.lower()\n",
    "all_lines = all_lines.replace('.', ' <PERIOD> ')\n",
    "all_lines = all_lines.replace(',', ' <COMMA> ')\n",
    "all_lines = all_lines.replace('\"', ' <QUOTATION_MARK> ')\n",
    "all_lines = all_lines.replace(';', ' <SEMICOLON> ')\n",
    "all_lines = all_lines.replace('!', ' <EXCLAMATION_MARK> ')\n",
    "all_lines = all_lines.replace('?', ' <QUESTION_MARK> ')\n",
    "all_lines = all_lines.replace('()', ' <LEFT_PAREN> ')\n",
    "all_lines = all_lines.replace(')', ' <RIGHT_PAREN> ')\n",
    "all_lines = all_lines.replace('--', ' <HYPHENS> ')\n",
    "all_lines = all_lines.replace(':', ' <COLON> ')\n",
    "\n",
    "# Get all words\n",
    "words = all_lines.split()\n",
    "\n",
    "print(words[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the vocabularys\n",
    "words_set = set(words)\n",
    "vocab_to_int = {word: id for word, id in zip(words, range(1, len(words) + 1))}\n",
    "int_to_vocab = {id: word for word, id in vocab_to_int.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
