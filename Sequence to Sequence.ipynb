{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we shall train a recurrent neural network on the [Cornell Movie dataset](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the version of the TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.2.1\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\n",
    "print('TensorFlow version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded the dataset!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Create the data folder if it doesn't exist\n",
    "if not os.path.exists(\"./data/\"):\n",
    "    os.makedirs('./data/')\n",
    "\n",
    "# Download the dataset\n",
    "url = 'http://www.mpi-sws.org/~cristian/data/cornell_movie_dialogs_corpus.zip'\n",
    "file_name = './data/{}'.format(os.path.basename(url))\n",
    "\n",
    "request = urlopen(url)\n",
    "data = request.read()\n",
    "\n",
    "with open(file_name, 'wb') as file:\n",
    "    file.write(data)\n",
    "\n",
    "# Unzip the dataset\n",
    "with ZipFile(file_name, 'r') as zf:\n",
    "    with zf.open('cornell movie-dialogs corpus/movie_conversations.txt', 'r') as source:\n",
    "        with open('./data/movie_conversations.txt', 'wb') as target:\n",
    "            target.write(source.read())\n",
    "    with zf.open('cornell movie-dialogs corpus/movie_lines.txt', 'r') as source:\n",
    "        with open('./data/movie_lines.txt', 'wb') as target:\n",
    "            target.write(source.read())\n",
    "\n",
    "# Delete the zip file\n",
    "os.remove(file_name)\n",
    "\n",
    "print('Downloaded the dataset!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting all the dialogues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here we have to join both text files in order to generate a list of all dialogues in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['L194', 'L195', 'L196', 'L197'], ['L198', 'L199'], ['L200', 'L201', 'L202', 'L203'], ['L204', 'L205', 'L206'], ['L207', 'L208']]\n"
     ]
    }
   ],
   "source": [
    "# Get all the sequence of conversations from the dataset\n",
    "with open('./data/movie_conversations.txt', 'rt') as file:\n",
    "    movie_conversations = [line.split(' +++$+++ ') for line in file.read().split('\\n')]\n",
    "    movie_conversations = [eval(line[3]) for line in movie_conversations if len(line) == 4]\n",
    "    \n",
    "print(movie_conversations[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['L1045', 'They do not!'], ['L1044', 'They do to!'], ['L985', 'I hope so.'], ['L984', 'She okay?'], ['L925', \"Let's go.\"]]\n"
     ]
    }
   ],
   "source": [
    "# Get all the movie lines from the dataset\n",
    "with open('./data/movie_lines.txt', 'rt') as file:\n",
    "    movie_lines = [line.split(' +++$+++ ') for line in file.read().split('\\n')]\n",
    "    movie_lines = [[line[0], line[4]] for line in movie_lines if len(line) == 5]\n",
    "\n",
    "print(movie_lines[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('L290794', \"I um... no, I don't think so...\"), ('L117719', \"Tommy, I'm bored shitless over here. What's up already?\"), ('L226197', \"That's your problem, Larry. That's why your sales are always below quota.  Your instinct to eat is stronger than your instinct to win.\"), ('L297057', 'What the hell are you kids doing down here?'), ('L201038', \"Great?  He's 17 -- you told her to stay away from him.\")]\n"
     ]
    }
   ],
   "source": [
    "# Transform the movie lines in a dictionary by the line ID\n",
    "lines_dict = {id: line for id, line in movie_lines}\n",
    "\n",
    "print(list(lines_dict.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.', \"Well, I thought we'd start with pronunciation, if that's okay with you.\", 'Not the hacking and gagging and spitting part.  Please.', \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\"], [\"You're asking me out.  That's so cute. What's your name again?\", 'Forget it.'], [\"No, no, it's my fault -- we didn't have a proper introduction ---\", 'Cameron.', \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\", 'Seems like she could get a date easy enough...']]\n"
     ]
    }
   ],
   "source": [
    "# Convert the conversations with their proper lines\n",
    "movie_dialogues = [[lines_dict[id] for id in line] for line in movie_conversations]\n",
    "\n",
    "print(movie_dialogues[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to replace tokens from some text\n",
    "def replace_tokens(text):\n",
    "    text = text.replace('.', ' <PERIOD> ')\n",
    "    text = text.replace(',', ' <COMMA> ')\n",
    "    text = text.replace('\"', ' <QUOTATION_MARK> ')\n",
    "    text = text.replace(';', ' <SEMICOLON> ')\n",
    "    text = text.replace('!', ' <EXCLAMATION_MARK> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    text = text.replace('()', ' <LEFT_PAREN> ')\n",
    "    text = text.replace(')', ' <RIGHT_PAREN> ')\n",
    "    text = text.replace('--', ' <HYPHENS> ')\n",
    "    text = text.replace(':', ' <COLON> ')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['can we make this quick <QUESTION_MARK>   roxanne korrine and andrew barrett are having an incredibly horrendous public break- up on the quad <PERIOD>   again <PERIOD> ', \"well <COMMA>  i thought we'd start with pronunciation <COMMA>  if that's okay with you <PERIOD> \", 'not the hacking and gagging and spitting part <PERIOD>   please <PERIOD> ', \"okay <PERIOD>  <PERIOD>  <PERIOD>  then how 'bout we try out some french cuisine <PERIOD>   saturday <QUESTION_MARK>   night <QUESTION_MARK> \"], [\"you're asking me out <PERIOD>   that's so cute <PERIOD>  what's your name again <QUESTION_MARK> \", 'forget it <PERIOD> '], [\"no <COMMA>  no <COMMA>  it's my fault  <HYPHENS>  we didn't have a proper introduction  <HYPHENS> -\", 'cameron <PERIOD> ', \"the thing is <COMMA>  cameron  <HYPHENS>  i'm at the mercy of a particularly hideous breed of loser <PERIOD>   my sister <PERIOD>   i can't date until she does <PERIOD> \", 'seems like she could get a date easy enough <PERIOD>  <PERIOD>  <PERIOD> ']]\n"
     ]
    }
   ],
   "source": [
    "# Lower the case and replace the tokens from all dialogues\n",
    "movie_dialogues = [[replace_tokens(line.lower()) for line in dialogue] for dialogue in movie_dialogues]\n",
    "\n",
    "print(movie_dialogues[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to create a vocabulary of all the words on the dialogues so that we can convert all words to their proper ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['can', 'we', 'make', 'this', 'quick', '<QUESTION_MARK>', 'roxanne', 'korrine', 'and', 'andrew', 'barrett', 'are', 'having', 'an', 'incredibly', 'horrendous', 'public', 'break-', 'up', 'on', 'the', 'quad', '<PERIOD>', 'again', '<PERIOD>', 'well', '<COMMA>', 'i', 'thought', \"we'd\", 'start', 'with', 'pronunciation', '<COMMA>', 'if', \"that's\", 'okay', 'with', 'you', '<PERIOD>', 'not', 'the', 'hacking', 'and', 'gagging', 'and', 'spitting', 'part', '<PERIOD>', 'please', '<PERIOD>', 'okay', '<PERIOD>', '<PERIOD>', '<PERIOD>', 'then', 'how', \"'bout\", 'we', 'try', 'out', 'some', 'french', 'cuisine', '<PERIOD>', 'saturday', '<QUESTION_MARK>', 'night', '<QUESTION_MARK>', \"you're\", 'asking', 'me', 'out', '<PERIOD>', \"that's\", 'so', 'cute', '<PERIOD>', \"what's\", 'your', 'name', 'again', '<QUESTION_MARK>', 'forget', 'it', '<PERIOD>', 'no', '<COMMA>', 'no', '<COMMA>', \"it's\", 'my', 'fault', '<HYPHENS>', 'we', \"didn't\", 'have', 'a', 'proper', 'introduction', '<HYPHENS>', '-', 'cameron', '<PERIOD>', 'the', 'thing', 'is', '<COMMA>', 'cameron', '<HYPHENS>', \"i'm\", 'at', 'the', 'mercy', 'of', 'a', 'particularly', 'hideous', 'breed', 'of', 'loser', '<PERIOD>', 'my', 'sister', '<PERIOD>', 'i', \"can't\", 'date', 'until', 'she', 'does', '<PERIOD>', 'seems', 'like', 'she', 'could', 'get', 'a', 'date', 'easy', 'enough', '<PERIOD>', '<PERIOD>', '<PERIOD>', 'why', '<QUESTION_MARK>', 'unsolved', 'mystery', '<PERIOD>', 'she', 'used', 'to', 'be', 'really', 'popular', 'when', 'she', 'started', 'high', 'school', '<COMMA>', 'then', 'it', 'was', 'just', 'like', 'she', 'got', 'sick', 'of', 'it', 'or', 'something', '<PERIOD>', \"that's\", 'a', 'shame', '<PERIOD>', 'gosh', '<COMMA>', 'if', 'only', 'we', 'could', 'find', 'kat', 'a', 'boyfriend', '<PERIOD>', '<PERIOD>', '<PERIOD>', 'let', 'me', 'see', 'what', 'i', 'can', 'do', '<PERIOD>', \"c'esc\"]\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all text\n",
    "all_lines = ' '.join([' '.join([line for line in dialogue]) for dialogue in movie_dialogues])\n",
    "\n",
    "# Get all words\n",
    "words = all_lines.split()\n",
    "\n",
    "print(words[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all the unique words\n",
    "words_set = set(words)\n",
    "\n",
    "# Create the vocabularys\n",
    "vocab_to_int = {word: id for word, id in zip(words_set, range(4, len(words_set) + 4))}\n",
    "vocab_to_int['<PAD>'] = 0\n",
    "vocab_to_int['<EOS>'] = 1\n",
    "vocab_to_int['<UNK>'] = 2\n",
    "vocab_to_int['<GO>'] = 3\n",
    "\n",
    "int_to_vocab = {id: word for word, id in vocab_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in te vocabulary: 65192\n"
     ]
    }
   ],
   "source": [
    "# Get total number of words in the vocabulary\n",
    "n_words = len(vocab_to_int)\n",
    "\n",
    "print('Number of words in te vocabulary: {}'.format(n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[28447, 18804, 54774, 62396, 49718, 22907, 14604, 42052, 46846, 47269, 8772, 27956, 2145, 42196, 41607, 63941, 38049, 27087, 42211, 6178, 26847, 45728, 3870, 39383, 3870], [53064, 30293, 13219, 57941, 54963, 24733, 58326, 49159, 30293, 21062, 34317, 35473, 58326, 11987, 3870], [5034, 26847, 36008, 46846, 23828, 46846, 7755, 53702, 3870, 5967, 3870], [35473, 3870, 3870, 3870, 29690, 35159, 30913, 18804, 20421, 43882, 49327, 9189, 37722, 3870, 48800, 22907, 8233, 22907]], [[55032, 11566, 54653, 43882, 3870, 34317, 16640, 12349, 3870, 30838, 50471, 40194, 39383, 22907], [47256, 3944, 3870]], [[29766, 30293, 29766, 30293, 5096, 52322, 8773, 49587, 18804, 4026, 10819, 36489, 10267, 48697, 49587, 18363], [63299, 3870], [26847, 52056, 27577, 30293, 63299, 49587, 12634, 61590, 26847, 38053, 31472, 36489, 59769, 15563, 2617, 31472, 51600, 3870, 52322, 11533, 3870, 13219, 26416, 63827, 18826, 16112, 9223, 3870], [15097, 26503, 16112, 30427, 6822, 36489, 63827, 10516, 10992, 3870, 3870, 3870]]]\n"
     ]
    }
   ],
   "source": [
    "# Convert the words in all dialogues to their ids\n",
    "movie_dialogues_int = [[[vocab_to_int[word] for word in line.split()] for line in dialogue] for dialogue in movie_dialogues]\n",
    "\n",
    "print(movie_dialogues_int[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what just happened <QUESTION_MARK> \n",
      "your daughters went to the prom <PERIOD> \n",
      "did i have anything to say about it <QUESTION_MARK> \n",
      "absolutely not <PERIOD> \n",
      "that ' s what i thought\n"
     ]
    }
   ],
   "source": [
    "for line in movie_dialogues[200]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Separate inputs and targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here we are going to construct the inputs and targets for the neural network. The inputs are going to be all the lines from the conversation, except the last one. And the targets are going to be all the lines starting by the second forward.\n",
    "\n",
    "For example, imagine if we have the following conversation:\n",
    "\n",
    "    what just happened?\n",
    "    your daughters went to the prom.\n",
    "    did i have anything to say about it? \n",
    "    absolutely not.\n",
    "    that ' s what i thought\n",
    "    \n",
    "The inputs would look like this:\n",
    "\n",
    "    [['what', 'just', 'happened', '<QUESTION_MARK>'],\n",
    "     ['your', 'daughters', 'went', 'to', 'the', 'prom', '<PERIOD>'],\n",
    "     ['did', 'i', 'have', 'anything', 'to', 'say', 'about', 'it', '<QUESTION_MARK>'],\n",
    "     ['absolutely', 'not', '<PERIOD>']]\n",
    "\n",
    "And the targets would look like this:\n",
    "\n",
    "    [['your', 'daughters', 'went', 'to', 'the', 'prom', '<PERIOD>'],\n",
    "     ['did', 'i', 'have', 'anything', 'to', 'say', 'about', 'it', '<QUESTION_MARK>'],\n",
    "     ['absolutely', 'not', '<PERIOD>'],\n",
    "     ['that's', 'what', 'i', 'thought']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for dialogue in movie_dialogues_int:\n",
    "    for line in dialogue[:-1]:\n",
    "        inputs.append(line)\n",
    "    for line in dialogue[1:]:\n",
    "        targets.append(line + [vocab_to_int['<EOS>']]) # append the <EOS> on the end of every target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53064, 30293, 13219, 57941, 54963, 24733, 58326, 49159, 30293, 21062, 34317, 35473, 58326, 11987, 3870]\n"
     ]
    }
   ],
   "source": [
    "print(inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[53064,\n",
       " 30293,\n",
       " 13219,\n",
       " 57941,\n",
       " 54963,\n",
       " 24733,\n",
       " 58326,\n",
       " 49159,\n",
       " 30293,\n",
       " 21062,\n",
       " 34317,\n",
       " 35473,\n",
       " 58326,\n",
       " 11987,\n",
       " 3870,\n",
       " 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "epochs = 100\n",
    "\n",
    "# Batch size\n",
    "batch_size = 128\n",
    "\n",
    "# RNN Size\n",
    "rnn_size = 50\n",
    "\n",
    "# Number of layers\n",
    "num_layers = 2\n",
    "\n",
    "# Embedding size\n",
    "encod_embed_size = 15\n",
    "decod_embed_size= 15\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_input():\n",
    "    with tf.name_scope('Input'):\n",
    "        inputs = tf.placeholder(tf.int32, shape=[None, None], name='input')\n",
    "        targets = tf.placeholder(tf.int32, shape=[None, None], name='target')\n",
    "        learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "        \n",
    "        source_sequence_length = tf.placeholder(tf.int32, shape=(None,), name='source_sequence_length')\n",
    "        target_sequence_length = tf.placeholder(tf.int32, shape=(None,), name='target_sequence_length')\n",
    "        max_target_sequence_length = tf.placeholder(tf.int32, name='max_target_sequence_length')\n",
    "        \n",
    "        return inputs, targets, learning_rate, source_sequence_length, target_sequence_length, max_target_sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoding_layer(input_data, rnn_size, num_layers, sequence_length, vocab_size, embed_size):\n",
    "    with tf.name_scope('Encoder'):\n",
    "        with tf.name_scope('Embedding'):\n",
    "            # Encoder Embedding\n",
    "            encod_embed = tf.contrib.layers.embed_sequence(input_data, vocab_size, embed_size)\n",
    "        \n",
    "        with tf.name_scope('RNN Cell'):\n",
    "            encod_cell = tf.contrib.rnn.LSTMCell(rnn_size, \n",
    "                                               nitializer=tf.truncated_normal_initializer(stddev=(1/math.sqrt(vocab_size))))\n",
    "            encod_cell = tf.contrib.rnn.MultiRNNCell([encod_cell] * num_layers)\n",
    "            \n",
    "            encod_output, encod_state = tf.nn.dynamic_rnn(encod_cell, encod_embed, \n",
    "                                                          sequence_length=sequence_length, dtype=tf.float32)\n",
    "    return encod_output, encod_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
